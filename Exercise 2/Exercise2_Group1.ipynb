{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f560da-b461-424f-bf1c-1dbeb09bee59",
   "metadata": {},
   "source": [
    "# **Exercise 2: Data Warehousing and Data Lakes with Spark + Hive**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In modern data engineering, we often encounter two primary paradigms:\n",
    "1. **Data Warehouse** (schema-on-write): where data is cleansed, transformed, and loaded into structured tables before analysis.\n",
    "2. **Data Lake** (schema-on-read): where data is stored in raw format and the schema is applied when querying.\n",
    "\n",
    "This exercise showcases both approaches using **Spark** and **Hive**. You will load and query **e-commerce data** in a structured (warehouse) format, then contrast this with a more flexible (lake) approach. By the end, you should understand key **ETL/ELT** concepts, the rationale behind each paradigm, and be able to discuss the differences.\n",
    "\n",
    "**Useful links and notebooks:**\n",
    "- https://spark.apache.org/docs/latest/api/python/index.html\n",
    "- https://spark.apache.org/docs/3.5.1/sql-data-sources-hive-tables.html\n",
    "- /shared/ETL_ELT\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "This exercise is worth 18 points. To earn full points, make sure to include comments in your code explaining your approach and the reasoning behind your choices.\n",
    "\n",
    "1. **Data Warehouse Fundamentals (6p)**:  \n",
    "   - Define and create schemas using Apache Hive.\n",
    "   - Perform schema-on-write transformations and run analytical queries.\n",
    "  \n",
    "     \n",
    "---\n",
    "2. **Data Lake Fundamentals (6p)**:  \n",
    "   - Ingest raw files into Spark without predefined schema (schema-on-read).  \n",
    "   - Handle multiple file formats (CSV, JSON, TXT) in a flexible manner.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Questions (6p)**:\n",
    "   - Answer three questions about the ETL and ELT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fee915-9db1-4d2f-ac0a-ad95f7995b35",
   "metadata": {},
   "source": [
    "# E-Commerce Data Schema\n",
    "\n",
    "We will be working with a couple of datasets from an e-commerce site located in the /shared folder.\n",
    "\n",
    "## 1. `customers.csv`\n",
    "- **Description**: Contains information about customers.\n",
    "- **Fields**:\n",
    "  - `customer_id` (int): Unique identifier for each customer.\n",
    "  - `name` (string): Customer's full name.\n",
    "  - `age` (int): Age of the customer.\n",
    "  - `country` (string): Country of residence.\n",
    "  - `preferred_category` (string): Preferred product category (e.g., Electronics, Books).\n",
    "  - `loyalty_score` (float): Loyalty score between 0.00 and 1.00.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `products.csv`\n",
    "- **Description**: Contains information about products.\n",
    "- **Fields**:\n",
    "  - `product_id` (int): Unique identifier for each product.\n",
    "  - `product_name` (string): Name of the product.\n",
    "  - `category` (string): Product category (e.g., Electronics, Clothing).\n",
    "  - `price` (float): Unit price of the product.\n",
    "  - `popularity` (int): Popularity score (1–10).\n",
    "  - `region` (string): Shipping region for the product (e.g., North America, Europe).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `transactions.json`\n",
    "- **Description**: Contains information about transactions.\n",
    "- **Fields**:\n",
    "  - `transaction_id` (int): Unique identifier for each transaction.\n",
    "  - `customer_id` (int): ID referencing a row in `customers.csv`.\n",
    "  - `product_id` (int): ID referencing a row in `products.csv`.\n",
    "  - `quantity` (int): Number of items purchased in the transaction.\n",
    "  - `price` (float): Unit price of the product.\n",
    "  - `shipping_cost` (float): Shipping cost for the transaction.\n",
    "  - `tax` (float): Tax amount applied to the transaction.\n",
    "  - `total_amount` (float): Computed total cost (`quantity * price + shipping_cost + tax`).\n",
    "  - `transaction_time` (string, ISO format): Timestamp of the transaction (e.g., `YYYY-MM-DDTHH:MM:SS`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `reviews.txt`\n",
    "- **Description**: Semi-structured text file containing product reviews.\n",
    "- **Format**: Each line follows the format: `customer_id|product_id|product_name|review_text|rating`.\n",
    "- **Fields**:\n",
    "  - `customer_id` (int): ID referencing a row in `customers.csv`.\n",
    "  - `product_id` (int): ID referencing a row in `products.csv`.\n",
    "  - `product_name` (string): Name of the reviewed product.\n",
    "  - `review_text` (string): Freeform text describing the customer’s opinion.\n",
    "  - `rating` (int): Numeric score (1–5).\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "- **Relationships**:\n",
    "  - `customer_id` links `transactions.json` and `reviews.txt` to `customers.csv`.\n",
    "  - `product_id` links `transactions.json` and `reviews.txt` to `products.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de1825-47ea-4382-990a-77912c9b888a",
   "metadata": {},
   "source": [
    "**Start by setting up a Spark session, enable Hive support so we can create databases and tables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039bf0ab-c9e4-4a79-af94-03ad9e9ebd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://np-faded-fuchsia-gloriosa-56b6f645f7-r4dc5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5d70185f50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exercise2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data_warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c84d86-43e2-4900-933c-8f4648283350",
   "metadata": {},
   "source": [
    "---\n",
    "# **1. ETL: Load data into a Data Warehouse (6p)**\n",
    "\n",
    "## Instructions\n",
    "1. Define the following tables:\n",
    "   - **`customers`**\n",
    "   - **`products`**\n",
    "   - **`transactions`**\n",
    "   - **`reviews`**\n",
    "2. Use **Parquet** format for optimized storage and query performance.\n",
    "3. Write `CREATE TABLE` statements in Hive to define the schema.\n",
    "4. **Optional**: Consider partitioning tables if you think it's reasonable, and explain the reasoning behind your decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73298bee-19ef-4feb-9df7-217633d98cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a database to store tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce\")\n",
    "spark.sql(\"USE ecommerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecf0f1f-5c46-4d84-aba2-0d4ff0d4e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases in Spark:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|ecommerce|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Databases in Spark:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64a5cf4-7cea-4612-9250-5610b1c4b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customers table - Partitioned by country because it is useful for queries with country as a filter\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce.customers (\n",
    "    customer_id INT,\n",
    "    name STRING,\n",
    "    age INT,\n",
    "    country STRING,\n",
    "    preferred_category STRING,\n",
    "    loyalty_score FLOAT\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (country)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1312d0a3-a093-4d85-99a3-1a8113d9a40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Products table - Partitioned by region because it is useful for queries with regions as a filter (customers from Europe will have Europe as a filter)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce.products (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    price FLOAT,\n",
    "    popularity INT,\n",
    "    region STRING\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (region)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cc87aa-a486-45a5-88bc-65b8f97bea11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transactions table - improves query performance for time-based analysis, which is the most common analysis for transactions\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce.transactions (\n",
    "    transaction_id INT,\n",
    "    customer_id INT,\n",
    "    product_id INT,\n",
    "    quantity INT,\n",
    "    price FLOAT,\n",
    "    shipping_cost FLOAT,\n",
    "    tax FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    transaction_time TIMESTAMP\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (transaction_time)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f4e56f-1df6-4ba4-a157-e1e6730df0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews table - improves speed for analysis of reviews by rating\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce.reviews (\n",
    "    customer_id INT,\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    review_text STRING,\n",
    "    rating INT\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (rating)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f6e35-888c-4cbe-aa29-6a292cc2a56a",
   "metadata": {},
   "source": [
    "### **ETL Process**\n",
    "\n",
    "Now that we have defined the tables we can extract raw data, clean it, and load it into the predefined tables.\n",
    "\n",
    "### Instructions\n",
    "1. Read raw data from the provided files located in the shared folder (`customers.csv`, `products.csv`, `transactions.json`, `reviews.txt`).\n",
    "2. Apply transformations:\n",
    "   - Cast columns to the correct data types.\n",
    "   - Handle missing or invalid data (e.g., filter out rows with null IDs, if such rows exist)\n",
    "   - Only insert the columns you find necessary.\n",
    "3. Use spark.sql or DataFrame APIs to insert the cleaned data into the warehouse tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cf6f78-5348-4e47-bb53-e6d7abc243cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from shared folder\n",
    "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.json(\"transactions.json\")\n",
    "reviews_rdd = spark.sparkContext.textFile(\"reviews.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d9ce0b-3755-4f0e-8025-f98373d9f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d845ec3-4dee-4e51-a24a-8599eabd04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean customers data\n",
    "# All columns kept\n",
    "cleaned_customers_df = (\n",
    "    customers_df\n",
    "    .filter(col(\"customer_id\").isNotNull())  # Filter out rows with null customer_id\n",
    "    .select(\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"name\").cast(\"string\"),\n",
    "        col(\"age\").cast(\"int\"),\n",
    "        col(\"country\").cast(\"string\"),\n",
    "        col(\"preferred_category\").cast(\"string\"),\n",
    "        col(\"loyalty_score\").cast(\"float\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fe62ca-e610-45fc-bfc5-1ee7bfb0842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean products data\n",
    "# All columns kept\n",
    "cleaned_products_df = (\n",
    "    products_df\n",
    "    .filter(col(\"product_id\").isNotNull())  # Filter out rows with null product_id\n",
    "    .select(\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"product_name\").cast(\"string\"),\n",
    "        col(\"category\").cast(\"string\"),\n",
    "        col(\"price\").cast(\"float\"),\n",
    "        col(\"popularity\").cast(\"int\"),\n",
    "        col(\"region\").cast(\"string\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67c3a49-9b70-4ffc-907f-f8f5d88bc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean transactions data\n",
    "# All columns kept\n",
    "cleaned_transactions_df = (\n",
    "    transactions_df\n",
    "    .filter(col(\"transaction_id\").isNotNull())  # Filter out rows with null transaction_id\n",
    "    .select(\n",
    "        col(\"transaction_id\").cast(\"int\"),\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"quantity\").cast(\"int\"),\n",
    "        col(\"price\").cast(\"float\"),\n",
    "        col(\"shipping_cost\").cast(\"float\"),\n",
    "        col(\"tax\").cast(\"float\"),\n",
    "        col(\"total_amount\").cast(\"float\"),\n",
    "        col(\"transaction_time\").cast(\"timestamp\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b865320-26ac-4637-b01a-66ae85ccf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse reviews data from RDD\n",
    "reviews_df = reviews_rdd.map(lambda line: line.split(\"|\")).toDF([\n",
    "    \"customer_id\", \"product_id\", \"product_name\", \"review_text\", \"rating\"\n",
    "])\n",
    "\n",
    "# Clean reviews data\n",
    "# All columns kept\n",
    "cleaned_reviews_df = (\n",
    "    reviews_df\n",
    "    .filter(col(\"customer_id\").isNotNull() & col(\"product_id\").isNotNull())  # Filter out rows with null IDs\n",
    "    .select(\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"product_name\").cast(\"string\"),\n",
    "        col(\"review_text\").cast(\"string\"),\n",
    "        col(\"rating\").cast(\"int\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43e45e5-18ab-4b38-9c22-36d090332e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to warehouse tables\n",
    "cleaned_customers_df.write.mode(\"overwrite\").insertInto(\"ecommerce.customers\")\n",
    "cleaned_products_df.write.mode(\"overwrite\").insertInto(\"ecommerce.products\")\n",
    "cleaned_transactions_df.write.mode(\"overwrite\").insertInto(\"ecommerce.transactions\")\n",
    "cleaned_reviews_df.write.mode(\"overwrite\").insertInto(\"ecommerce.reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94426c0-fbf0-45d8-af10-8a09027d2608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---+--------------+------------------+-------------+\n",
      "|customer_id|             name|age|       country|preferred_category|loyalty_score|\n",
      "+-----------+-----------------+---+--------------+------------------+-------------+\n",
      "|          1|    Cindy Simpson| 60|United Kingdom|          Clothing|         0.15|\n",
      "|          2|       Eric White| 41|United Kingdom|          Clothing|         0.22|\n",
      "|          3|       Linda Todd| 54|United Kingdom|              Home|          0.5|\n",
      "|          4|    Shannon Woods| 52|        Canada|            Sports|         0.71|\n",
      "|          5|    Michael Brown| 48|        France|          Clothing|         0.36|\n",
      "|          6|Priscilla Stewart| 72|        France|             Books|         0.06|\n",
      "|          7|    Katie Allison| 24|United Kingdom|       Electronics|         0.04|\n",
      "|          8|     Jeremy Weiss| 73| United States|             Books|         0.11|\n",
      "|          9| Shelly Castaneda| 59|        Canada|            Sports|         0.13|\n",
      "|         10|      Karen Jones| 45|        France|            Sports|         0.44|\n",
      "+-----------+-----------------+---+--------------+------------------+-------------+\n",
      "\n",
      "+----------+-----------------+-----------+------+----------+-------------+\n",
      "|product_id|     product_name|   category| price|popularity|       region|\n",
      "+----------+-----------------+-----------+------+----------+-------------+\n",
      "|         1|         Raincoat|   Clothing| 43.99|        10|North America|\n",
      "|         2|         Sneakers|   Clothing| 25.99|         6|       Europe|\n",
      "|         3|   Self-Help Book|      Books| 48.99|         6|       Europe|\n",
      "|         4|    Action Camera|Electronics|680.99|         7|North America|\n",
      "|         5|       4K Monitor|Electronics|824.99|         5|North America|\n",
      "|         6|     Dumbbell Set|     Sports|229.99|         8|       Europe|\n",
      "|         7|  Mattress Topper|       Home|467.99|         1|North America|\n",
      "|         8|         Curtains|       Home|128.99|         2|       Europe|\n",
      "|         9| Resistance Bands|     Sports| 83.99|         4|North America|\n",
      "|        10|Programming Guide|      Books| 46.99|         3|       Europe|\n",
      "+----------+-----------------+-----------+------+----------+-------------+\n",
      "\n",
      "+--------------+-----------+----------+--------+------+-------------+-----+------------+--------------------+\n",
      "|transaction_id|customer_id|product_id|quantity| price|shipping_cost|  tax|total_amount|    transaction_time|\n",
      "+--------------+-----------+----------+--------+------+-------------+-----+------------+--------------------+\n",
      "|             1|         32|        22|       3| 77.99|        11.03| 23.4|       268.4|2024-11-09 09:48:...|\n",
      "|             2|         40|        40|       1|384.99|        13.44| 38.5|      436.93|2024-08-06 08:26:...|\n",
      "|             3|         10|        18|       1|174.99|        19.44| 17.5|      211.93|2024-02-26 12:33:...|\n",
      "|             4|         91|        20|       5| 19.99|        11.71| 9.99|      121.65|2024-08-15 16:23:...|\n",
      "|             5|         86|        42|       2|161.99|         15.7| 32.4|      372.08|2024-03-03 21:44:...|\n",
      "|             6|         90|        21|       5| 74.99|        19.17|37.49|      431.61|2024-09-14 00:39:...|\n",
      "|             7|         96|        49|       3|263.99|         8.02| 79.2|      879.19|2024-08-10 07:33:...|\n",
      "|             8|         47|        11|       2| 66.99|         5.19| 13.4|      152.57|2024-02-10 04:48:...|\n",
      "|             9|         78|         5|       1|824.99|         6.78| 82.5|      914.27|2024-03-22 16:02:...|\n",
      "|            10|         71|        16|       1|185.99|         8.86| 18.6|      213.45|2024-10-26 06:17:...|\n",
      "+--------------+-----------+----------+--------+------+-------------+-----+------------+--------------------+\n",
      "\n",
      "+-----------+----------+----------------+--------------------+------+\n",
      "|customer_id|product_id|    product_name|         review_text|rating|\n",
      "+-----------+----------+----------------+--------------------+------+\n",
      "|         92|         6|    Dumbbell Set|Fantastic build q...|     5|\n",
      "|         17|        50|Wireless Earbuds|A premium product...|     4|\n",
      "|         34|         8|        Curtains|Exceeded my expec...|     5|\n",
      "|          7|        46|   Exercise Bike|High-quality and ...|     4|\n",
      "|         35|        31|    Water Bottle|A premium product...|     4|\n",
      "|          4|        18|   Tennis Racket|Does the job, but...|     3|\n",
      "|         57|        46|   Exercise Bike|Exceeded my expec...|     4|\n",
      "|         76|        32|  Vacuum Cleaner|Decent quality fo...|     3|\n",
      "|         75|        42|        Yoga Mat|Good for the pric...|     3|\n",
      "|         22|        17|     Formal Suit|Absolutely worth ...|     5|\n",
      "+-----------+----------+----------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check table contents\n",
    "spark.sql(\"SELECT * FROM ecommerce.customers LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM ecommerce.products LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM ecommerce.transactions LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM ecommerce.reviews LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560b09-da81-4679-9256-04c0be4510a2",
   "metadata": {},
   "source": [
    "## Analyze the Data\n",
    "\n",
    "## Objective\n",
    "Run SQL queries to analyze the transformed data.\n",
    "\n",
    "### Example Queries to Run\n",
    "\n",
    "1. **Total Revenue and Transactions per Product Category**  \n",
    "  \n",
    "\n",
    "2. **Identify the 5 Least Sold Products**  \n",
    "  \n",
    "\n",
    "3. **Identify the Top 5 Spending Customers**  \n",
    "\n",
    "\n",
    "**You are encouraged to run these queries, but feel free to explore the data and create your own queries if you believe they provide better insights or are more relevant for analysis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d89538-e974-4ce0-b4d3-6fce97626f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+\n",
      "|   category|     total_revenue|num_of_transactions|\n",
      "+-----------+------------------+-------------------+\n",
      "|   Clothing|21233.000051498413|                300|\n",
      "|      Books| 6439.710102081299|                229|\n",
      "|Electronics| 83316.10888671875|                189|\n",
      "|     Sports|31397.290817260742|                171|\n",
      "|       Home|27461.890014648438|                111|\n",
      "+-----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Revenue and Transactions per Product Category\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.category, SUM(t.price) as total_revenue, COUNT(transaction_id) as num_of_transactions\n",
    "    FROM transactions t\n",
    "    JOIN products p\n",
    "    ON t.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY num_of_transactions DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "551cdf31-24da-4c2a-8337-967ec88153d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|        product_name|total_quantity_sold|\n",
      "+--------------------+-------------------+\n",
      "|        Air Purifier|                  5|\n",
      "|          Wall Clock|                  5|\n",
      "|        Coffee Maker|                  5|\n",
      "|       Action Camera|                  7|\n",
      "|Noise-Canceling H...|                  9|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the 5 Least Sold Products\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.product_name, SUM(t.quantity) as total_quantity_sold\n",
    "    FROM transactions t\n",
    "    JOIN products p\n",
    "    ON t.product_id = p.product_id\n",
    "    GROUP BY p.product_name\n",
    "    ORDER BY total_quantity_sold ASC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd0125f-80c5-4eb5-8cf0-61c268c2c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            name| total_money_spent|\n",
      "+----------------+------------------+\n",
      "|     Nancy Jones|   15662.419921875|\n",
      "|     Cesar Davis| 14997.11003112793|\n",
      "|Valerie Mitchell|14160.320007324219|\n",
      "|  Anthony Pruitt|12767.850021362305|\n",
      "|  Nicholas Davis|11635.649841308594|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the Top 5 Spending Customers\n",
    "spark.sql(\"\"\"\n",
    "    SELECT c.name, SUM(t.total_amount) as total_money_spent\n",
    "    FROM transactions t\n",
    "    JOIN customers c\n",
    "    ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_money_spent DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "444b13fe-7f7d-4a8a-8812-9d37c265ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|   category|        avg_rating|\n",
      "+-----------+------------------+\n",
      "|Electronics| 4.194444444444445|\n",
      "|     Sports|3.6315789473684212|\n",
      "|       Home|3.5813953488372094|\n",
      "|   Clothing|3.1463414634146343|\n",
      "|      Books| 3.119047619047619|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Product categories ranked by ratings\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.category, AVG(r.rating) as avg_rating\n",
    "    FROM reviews r\n",
    "    JOIN products p\n",
    "    ON r.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY avg_rating DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193d66-78d0-4989-b8c0-5bb2c4407a70",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. **ELT: Load Raw Data into a Data Lake (6p)**\n",
    "\n",
    "## Objective\n",
    "Copy the raw data files into a `data_lake/` directory and transform the data on read.\n",
    "\n",
    "### Instructions\n",
    "1. Copy or use shell commands or scripts to move the files into a `data_lake/` directory in your `my-work` folder.\n",
    "2. Do not modify the files; load them “as is” to retain their raw state.\n",
    "\n",
    "Now the `data_lake/` folder contains all raw files, unmodified:\n",
    "\n",
    "```plaintext\n",
    "data_lake/\n",
    "├── customers.csv\n",
    "├── products.csv\n",
    "├── transactions.json\n",
    "└── reviews.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55bc2a9f-271b-4eeb-9868-bb8532bf9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p data_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92b7b0dc-018c-43a4-a620-2c6dc2237fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied to data_lake/ successfully!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "source_dir = \"./\"\n",
    "destination_dir = \"data_lake/\"\n",
    "\n",
    "# Ensure destination directory exists\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# List of files to copy\n",
    "files = [\"customers.csv\", \"products.csv\", \"transactions.json\", \"reviews.txt\"]\n",
    "\n",
    "# Copy files\n",
    "for file in files:\n",
    "    shutil.copy(os.path.join(source_dir, file), destination_dir)\n",
    "\n",
    "print(\"Files copied to data_lake/ successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab7bb199-59d3-4e85-bc51-ea1ba0f252f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 216\n",
      "-rw-r--r--. 1 jovyan root   4430 Jan 28 17:19 customers.csv\n",
      "-rw-r--r--. 1 jovyan root   2235 Jan 28 17:19 products.csv\n",
      "-rw-r--r--. 1 jovyan root  20147 Jan 28 17:19 reviews.txt\n",
      "-rw-r--r--. 1 jovyan root 185370 Jan 28 17:19 transactions.json\n"
     ]
    }
   ],
   "source": [
    "ls -l data_lake/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf88f40-eadf-4c2b-bdda-ff198508b350",
   "metadata": {},
   "source": [
    "# **Transform and Analyze**\n",
    "\n",
    "### Instructions\n",
    "1. Read the raw files from the `data_lake/` directory using Spark.\n",
    "2. Clean and transform the data on read.\n",
    "3. Register the transformed DataFrames as temporary views.\n",
    "4. Run the same queries as in the warehouse approach:\n",
    "   \n",
    "- Total Revenue and Transactions per Product Category\n",
    "  \n",
    "- Identify the 5 Least Sold Products\n",
    "  \n",
    "- Identify the Top 5 Customers by Spending\n",
    "\n",
    "\n",
    "**You are encouraged to run these queries, but feel free to explore the data and create your own queries if you believe they provide better insights or are more relevant for analysis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdc649ff-e031-409a-96d5-fa2e93abde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from data_lake folder\n",
    "customers_df = spark.read.csv(\"data_lake/customers.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(\"data_lake/products.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.json(\"data_lake/transactions.json\")\n",
    "reviews_rdd = spark.sparkContext.textFile(\"data_lake/reviews.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fd87d1f-d72b-47b2-9a7c-42b20b59923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean customers data\n",
    "cleaned_customers_df = (\n",
    "    customers_df\n",
    "    .filter(col(\"customer_id\").isNotNull())  # Filter out rows with null customer_id\n",
    "    .select(\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"name\").cast(\"string\"),\n",
    "        col(\"age\").cast(\"int\"),\n",
    "        col(\"country\").cast(\"string\"),\n",
    "        col(\"preferred_category\").cast(\"string\"),\n",
    "        col(\"loyalty_score\").cast(\"float\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b77e3635-b5f1-4a06-886f-1b7b639b2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean products data\n",
    "cleaned_products_df = (\n",
    "    products_df\n",
    "    .filter(col(\"product_id\").isNotNull())  # Filter out rows with null product_id\n",
    "    .select(\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"product_name\").cast(\"string\"),\n",
    "        col(\"category\").cast(\"string\"),\n",
    "        col(\"price\").cast(\"float\"),\n",
    "        col(\"popularity\").cast(\"int\"),\n",
    "        col(\"region\").cast(\"string\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c851febc-b56b-454a-a729-207a6b3cc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean transactions data\n",
    "cleaned_transactions_df = (\n",
    "    transactions_df\n",
    "    .filter(col(\"transaction_id\").isNotNull())  # Filter out rows with null transaction_id\n",
    "    .select(\n",
    "        col(\"transaction_id\").cast(\"int\"),\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"quantity\").cast(\"int\"),\n",
    "        col(\"price\").cast(\"float\"),\n",
    "        col(\"shipping_cost\").cast(\"float\"),\n",
    "        col(\"tax\").cast(\"float\"),\n",
    "        col(\"total_amount\").cast(\"float\"),\n",
    "        col(\"transaction_time\").cast(\"timestamp\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3ba0265-52e6-445d-99de-da5ca0ba7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse reviews data from RDD\n",
    "reviews_df = reviews_rdd.map(lambda line: line.split(\"|\")).toDF([\n",
    "    \"customer_id\", \"product_id\", \"product_name\", \"review_text\", \"rating\"\n",
    "])\n",
    "\n",
    "# Clean reviews data\n",
    "cleaned_reviews_df = (\n",
    "    reviews_df\n",
    "    .filter(col(\"customer_id\").isNotNull() & col(\"product_id\").isNotNull())  # Filter out rows with null IDs\n",
    "    .select(\n",
    "        col(\"customer_id\").cast(\"int\"),\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"product_name\").cast(\"string\"),\n",
    "        col(\"review_text\").cast(\"string\"),\n",
    "        col(\"rating\").cast(\"int\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe3476b2-4362-4022-a735-3c7a9742add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary views\n",
    "cleaned_customers_df.createOrReplaceTempView(\"customers_view\")\n",
    "cleaned_products_df.createOrReplaceTempView(\"products_view\")\n",
    "cleaned_transactions_df.createOrReplaceTempView(\"transactions_view\")\n",
    "cleaned_reviews_df.createOrReplaceTempView(\"reviews_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d0e064a-954a-4981-b071-fbf6c05a12a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+\n",
      "|   category|     total_revenue|num_of_transactions|\n",
      "+-----------+------------------+-------------------+\n",
      "|   Clothing|21233.000051498413|                300|\n",
      "|      Books| 6439.710102081299|                229|\n",
      "|Electronics| 83316.10888671875|                189|\n",
      "|     Sports|31397.290817260742|                171|\n",
      "|       Home|27461.890014648438|                111|\n",
      "+-----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Total Revenue and Transactions per Product Category\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.category, SUM(t.price) as total_revenue, COUNT(transaction_id) as num_of_transactions\n",
    "    FROM transactions_view t\n",
    "    JOIN products_view p\n",
    "    ON t.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY num_of_transactions DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fe0d1c9-0c9a-4825-810d-b782a0e7911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|        product_name|total_quantity_sold|\n",
      "+--------------------+-------------------+\n",
      "|        Air Purifier|                  5|\n",
      "|          Wall Clock|                  5|\n",
      "|        Coffee Maker|                  5|\n",
      "|       Action Camera|                  7|\n",
      "|Noise-Canceling H...|                  9|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the 5 Least Sold Products\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.product_name, SUM(t.quantity) as total_quantity_sold\n",
    "    FROM transactions_view t\n",
    "    JOIN products_view p\n",
    "    ON t.product_id = p.product_id\n",
    "    GROUP BY p.product_name\n",
    "    ORDER BY total_quantity_sold ASC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07940593-4beb-4186-98d4-81dd2cd1deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            name| total_money_spent|\n",
      "+----------------+------------------+\n",
      "|     Nancy Jones|   15662.419921875|\n",
      "|     Cesar Davis| 14997.11003112793|\n",
      "|Valerie Mitchell|14160.320007324219|\n",
      "|  Anthony Pruitt|12767.850021362305|\n",
      "|  Nicholas Davis|11635.649841308594|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the Top 5 Customers by Spending\n",
    "spark.sql(\"\"\"\n",
    "    SELECT c.name, SUM(t.total_amount) as total_money_spent\n",
    "    FROM transactions_view t\n",
    "    JOIN customers_view c\n",
    "    ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_money_spent DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79de04a0-d12e-4147-9200-99bd6d98784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|   category|        avg_rating|\n",
      "+-----------+------------------+\n",
      "|Electronics| 4.194444444444445|\n",
      "|     Sports|3.6315789473684212|\n",
      "|       Home|3.5813953488372094|\n",
      "|   Clothing|3.1463414634146343|\n",
      "|      Books| 3.119047619047619|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Product categories ranked by ratings\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.category, AVG(r.rating) as avg_rating\n",
    "    FROM reviews_view r\n",
    "    JOIN products_view p\n",
    "    ON r.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY avg_rating DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644e99d-5d1f-4e67-a338-a4a94a417df0",
   "metadata": {},
   "source": [
    "---\n",
    "### **Questions (6p)**\n",
    "\n",
    "Reflect on the following questions and provide thoughtful answers. Focus on your reasoning, insights, and key takeaways from the exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2c4a4-67e4-431d-bf5f-5f61fe1697cb",
   "metadata": {},
   "source": [
    "**1. What were the key differences in how data was handled and queried in the warehouse (ETL) versus the lake (ELT)? Which approach felt more adaptable to changes in data structure or format, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48512e-7ead-4ace-9269-9e917b48c5b6",
   "metadata": {},
   "source": [
    "When working with the warehouse (ETL) approach, the data was cleaned, transformed, and structured before loading it into predefined tables. This ensured the data was consistent and ready for analysis, with a fixed schema. Queries ran smoothly because the data was already cleaned and optimized for performance.\n",
    "\n",
    "On the other hand, with the lake (ELT) approach, the raw files were directly loaded into the data lake without modifying them. Transformations and schema were applied dynamically at query time. This was more flexible because the raw data remained as raw, allowing for adjustment of queries or transformations as needed.\n",
    "\n",
    "Given the explanations above, the ELT approach appears to be more adaptable. Since the raw data was stored without enforcing a schema, the changes in data structure or format could be handled easily. For example, if new fields were added or the file format was changed, the transformations or queries could be simply updated without reloading the data. However, the schema needs to be defined upfront in ETL approach, which makes that approach less flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906df7f7-59d4-4778-93e8-2083f454e063",
   "metadata": {},
   "source": [
    "**2. What challenges did you encounter when transforming and querying the data in each approach? How did these challenges help you better understand the trade-offs of schema-on-write vs. schema-on-read?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea2e69-19a2-42b6-8c21-f09898385ed3",
   "metadata": {},
   "source": [
    "In ETL, the data needed to be cleaned and filtered (removing rows with missing values, matching the data types defined in the tables) to match the predefined schema, otherwise it wouldn't be inserted into tables.\n",
    "In ELT, the approach is more flexible but transforming the data dynamically during querying requires extra effort to handle raw files, and extra time -> performance might be affected. This may not me noticable for small datasets, but for larger ones constantly applying transformations when querying can impact the performance. \n",
    "The trade-offs are very general, if the goal is to care less about the data structure and load it as fast as possible, then querying will be less efficient and take more time. On the other hand, if we want to spend more time working on processing the data before loading it in tables, the query performance will be optimized afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d204c1d-4633-4059-84d2-0f63ad6a85b8",
   "metadata": {},
   "source": [
    "**3. What factors would you consider when deciding between a warehouse, a lake, or a hybrid approach for a real-world data solution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103b195-d22c-42b6-89b0-c3dbbcac9703",
   "metadata": {},
   "source": [
    "A data warehouse is better for structured data, such as transactional records, or data where schema is constant and will not change in the future, where schema consistency and high-performance querying are essential. On the other hand, a data lake is better for handling unstructured and semi-structured data, such as images, videos etc. If we are dealing with a combination of both structured and unstructured data, a hybrid approach can be used.\n",
    "\n",
    "DW is better for BI, reporting and tasks which require repeatable queries. Data lake is better for analysis, machine learning and big data operations since flexibility is needed for such cases. When both types of analysis are needed, it would be better to use the hybrid approach.\n",
    "\n",
    "DW requires higher costs but it performs fast. On the other hand, lakes work slower but with lower costs. So depending on the trade off between cost and performance (what is the most important for the task at hand), one can choose one of the approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
